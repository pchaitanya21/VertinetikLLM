{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4133f8e2-c4b1-42cc-bcae-22f0827bebea",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59f6cb36-7799-4584-bef0-d20842f857c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import requests\n",
    "# import networkx as nx\n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# from pyvis.network import Network\n",
    "# import openai\n",
    "from IPython.display import display, HTML, Code\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc124b8-7809-4684-afb0-1373919791e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import LLM_Geo_Constants as constants\n",
    "import helper\n",
    "# import LLM_Geo_kernel.Solution as Solution\n",
    "\n",
    "from LLM_Geo_kernel import Solution\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabf375c-14ce-4468-95c0-9ea43ad096d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9a91ae-5723-456d-beb1-14e88377d5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt to get solution graph:\n",
      "\n",
      "Your role: A professional Geo-information scientist and programmer good at Python. You have worked on Geographic information science more than 20 years, and know every detail and pitfall when processing spatial data and coding. Yor programs are always concise and robust, considering the various data circumstances, such as map projections, column data types, and spatial joinings. You are also super experienced on generating map.\n",
      " \n",
      "Your task: Write a Python program to address the question or task: 1) Draw a choropleth map to show the death rate (death/case) of COVID-19 among the countiguous US counties. Use the accumulated COVID-19 data of 2020.12.31 to compute the death rate. Use scheme ='quantiles' when plotting the map.  Set map projection to 'Conus Albers'. Set map size to 15*10 inches.  \n",
      "2) Draw a scatter plot to show the correlation and trend line of the death rate with the senior resident rate, including the r-square and p-value. Set data point transparency to 50%, regression line as red.  Set figure size to 15*10 inches.  \n",
      " \n",
      "Location for data you may need: 1. COVID-19 data case in 2020 (county-level): https://github.com/nytimes/covid-19-data/raw/master/us-counties-2020.csv. This data is for daily accumulated COVID cases and deaths for each county in the US. There are 5 columns: date (format: 2021-02-01), county, state, fips, cases, deaths. \n",
      "2. Contiguous US county boundary (ESRI shapefile): https://github.com/gladcolor/spatial_data/raw/master/contiguous_counties.zip. The county FIPS column is 'GEOID'; map projection is EPSG:4269\n",
      "3. Census data (ACS2020): https://raw.githubusercontent.com/gladcolor/spatial_data/master/Demography/ACS2020_5year_county.csv. THe needed columns are: 'FIPS', 'Total Population', 'Total Population: 65 to 74 Years', 'Total Population: 75 to 84 Years', 'Total Population: 85 Years and Over'. Drop rows with NaN cells after loading the used columns. \n",
      "Your reply needs to meet these requirements: \n",
      " 1. You can think step by step.\n",
      "2. DO NOT change the given variable names and paths.\n",
      "3. Put your reply into a Python code block(enclosed by ```python and ```), NO explanation or conversation outside the code block.\n",
      "4. If using GeoPandas to load a zipped ESRI shapefile from a URL, the correct method is \"gpd.read_file(URL)\". DO NOT download and unzip the file.\n",
      "5. Generate descriptions for input and output arguments.\n",
      "6. Note module 'pandas' has no attribute or method of 'StringIO'.\n",
      "7. Use the latest Python modules and methods.\n",
      "8. When doing spatial analysis, convert the involved spatial layers into the same map projection, if they are not in the sample projection.\n",
      "9. Map projection conversion is only conducted for spatial data layers such as GeoDataFrame. DataFrame loaded from a CSV file does not have map projection information.\n",
      "10. If join DataFrame and GeoDataFrame, using common columns, DO NOT convert DataFrame to GeoDataFrame.\n",
      "11. Graphs or maps need to show the unit, legend, or colorbar.\n",
      "12. Remember the variable, column, and file names used in ancestor functions when reusing them, such as joining tables or calculating.\n",
      "13. If using GeoPandas for spatial analysis, when doing overlay analysis, carefully think about use Geopandas.GeoSeries.intersects() or geopandas.sjoin(). \n",
      "14. Geopandas.GeoSeries.intersects(other, align=True) returns a Series of dtype('bool') with value True for each aligned geometry that intersects other. other:GeoSeries or geometric object. \n",
      "15. If using GeoPandas for spatial joining, the arguements are: geopandas.sjoin(left_df, right_df, how='inner', predicate='intersects', lsuffix='left', rsuffix='right', **kwargs), how: the type of join, default ‘inner’, means use intersection of keys from both dfs while retain only left_df geometry column. If 'how' is 'left': use keys from left_df; retain only left_df geometry column, and similarly when 'how' is 'right'. \n",
      "16. Note geopandas.sjoin() returns all joined pairs, i.e., the return could be one-to-many. E.g., the intersection result of a polygon with two points inside it contains two rows; in each row, the polygon attribute is the same. If you need of extract the polygons intersecting with the points, please remember to remove the duplicated rows in the results.\n",
      "17. The program is executable, put it in a function named 'direct_solution()' then run it, but DO NOT use 'if __name__ == '__main__:' statement because this program needs to be executed by exec().\n",
      "18. Before using Pandas or GeoPandas columns for further processing (e.g. join or calculation), drop recoreds with NaN cells in that column, i.e., df.dropna(subset=['XX', 'YY']).\n",
      "19. When read FIPS or GEOID columns from CSV files, read those columns as str or int, never as float.\n",
      "20. FIPS or GEOID columns may be str type with leading zeros (digits: state: 2, county: 5, tract: 11, block group: 12), or integer type without leading zeros. Thus, when joining they, you can convert the integer colum to str type with leading zeros to ensure the success.\n",
      "21. If you need to make a map and the map size is not given, set the map size to 15*10 inches. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #Case 1: population living near hazardous wastes\n",
    "\n",
    "# task_name ='Resident_at_risk_counting'\n",
    "\n",
    "# TASK = r\"\"\"1) Find out the total population that lives within a tract that contain hazardous waste facilities. The study area is North Carolina, US.\n",
    "# 2) Generate a map to show the spatial distribution of population at the tract level and highlight the borders of tracts that have hazardous waste facilitiesm, also show the facility location.\n",
    "# \"\"\"\n",
    "# DATA_LOCATIONS = [\"NC hazardous waste facility ESRI shape file location: https://github.com/gladcolor/LLM- Geo/raw/master/overlay_analysis/Hazardous_Waste_Sites.zip.\",\n",
    "#                   \"NC tract boundary shapefile location: https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/tract_shp_37.zip. The tract id column is 'Tract'.\",\n",
    "#                   \"NC tract population CSV file location: https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/NC_tract_population.csv. The population is stored in 'TotalPopulation' column. The tract ID column is 'GEOID'.\"\n",
    "#                  ]\n",
    "\n",
    "\n",
    "# Case 2: France_mobility_changes_2020\n",
    "# task_name ='France_mobility_changes_2020'\n",
    "# TASK = r'''\n",
    "# 1) Show the 2020 human mobility monthly change rates of each administrative regions in a France choropleth map. Each month is a sub-map in a map matrix，12 months in total. All monthly maps need to use the same colorbar range. The base of the change rate is January 2020. \n",
    "# 2) Draw a line chart to show the monthly change rate trends of all administrative regeions. Each region is a line (the region name is the legend), the x-axis is 2020 months.\n",
    "# '''\n",
    "\n",
    "# DATA_LOCATIONS = [\"ESRI shapefile for France administrative regions:\" + \\\n",
    "#                   \"https://github.com/gladcolor/LLM-Geo/raw/master/REST_API/France.zip. \" + \\\n",
    "#                   \"The 'GID_1' column is the administrative region code, 'NAME_1' column is the administrative region name.\",\n",
    "#                   \"REST API url with parameters for daily human mobility data access:\" + \\\n",
    "#                   \"http://gis.cas.sc.edu/GeoAnalytics/REST?operation=get_daily_movement_for_all_places&source=twitter&scale=world_first_level_admin&begin=01/01/2020&end=12/31/2020.\" + \\\n",
    "#                   \"The response is in CSV format. There are three columns in the response: \" + \\\n",
    "#                   \"place,date (format:2020-01-07), and intra_movement. 'place' column is the administractive region code of every country;\" + \\\n",
    "#                   \"codes for France administrative regions start with 'FRA'. Use the total intra_movement of the month as the montly mobility.\",\n",
    "#                  ]\n",
    "\n",
    "\n",
    "# # Case 3: COVID-19 death rate prevalence trend\n",
    "task_name ='COVID_death_rate'\n",
    "TASK = r'''1) Draw a choropleth map to show the death rate (death/case) of COVID-19 among the countiguous US counties. Use the accumulated COVID-19 data of 2020.12.31 to compute the death rate. Use scheme ='quantiles' when plotting the map.  Set map projection to 'Conus Albers'. Set map size to 15*10 inches.  \n",
    "2) Draw a scatter plot to show the correlation and trend line of the death rate with the senior resident rate, including the r-square and p-value. Set data point transparency to 50%, regression line as red.  Set figure size to 15*10 inches.  \n",
    "'''\n",
    "\n",
    "\n",
    "DATA_LOCATIONS = [\n",
    "                  r\"COVID-19 data case in 2020 (county-level): https://github.com/nytimes/covid-19-data/raw/master/us-counties-2020.csv. This data is for daily accumulated COVID cases and deaths for each county in the US. There are 5 columns: date (format: 2021-02-01), county, state, fips, cases, deaths. \",   \n",
    "                  r\"Contiguous US county boundary (ESRI shapefile): https://github.com/gladcolor/spatial_data/raw/master/contiguous_counties.zip. The county FIPS column is 'GEOID'; map projection is EPSG:4269\",\n",
    "                  r\"Census data (ACS2020): https://raw.githubusercontent.com/gladcolor/spatial_data/master/Demography/ACS2020_5year_county.csv. THe needed columns are: 'FIPS', 'Total Population', 'Total Population: 65 to 74 Years', 'Total Population: 75 to 84 Years', 'Total Population: 85 Years and Over'. Drop rows with NaN cells after loading the used columns.\",\n",
    "                 ]\n",
    "\n",
    " \n",
    "\n",
    "# # # Case 4: Hospital_accessibility\n",
    "# task_name ='Hospital_accessibility'\n",
    "# TASK = r'''\n",
    "# For each zipcode area in South Carolina (SC), calculate the distance from the centroid of the zipcode area to its nearest hospital, and then create a choropleth distance map of zipcode area polygons (unit: km), also show the hospital.\n",
    "# '''\n",
    "\n",
    "# DATA_LOCATIONS = [\n",
    "# r\"SC zipcode boundary shapefile: https://github.com/GIBDUSC/test/raw/master/sc_zip_boundary.zip, the map projection is WGS1984.\",\n",
    "# r\"SC hospitals:  https://github.com/gladcolor/spatial_data/raw/master/South_Carolina/SC_hospitals_with_emergency_room_cleaned.csv, location columns: longitude in 'POINT_X' column, latitude in 'POINT_Y' column.\",          \n",
    "# ]\n",
    "\n",
    "\n",
    "## Case 5: Census API\n",
    "# task_name ='Census_API'\n",
    "# TASK = r''' Show the spatial distribution of the county level median income in the contigous US. Set figure size to (25,15)\n",
    "# '''\n",
    "\n",
    "# DATA_LOCATIONS = [\n",
    "#       \"You can use the Census API.\",\n",
    "# ]\n",
    "\n",
    "# '''Census API key: ae7be70727932dd6aed257692de3f344365d0678'''        \n",
    "\n",
    " \n",
    "save_dir = os.path.join(os.getcwd(), task_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# create graph\n",
    "# model=r\"gpt-3.5-turbo\"\n",
    "model=\"gemini-1.5-flash-002\"\n",
    "solution = Solution(\n",
    "                    task=TASK,\n",
    "                    task_name=task_name,\n",
    "                    save_dir=save_dir,\n",
    "                    data_locations=DATA_LOCATIONS,\n",
    "                    model=model,\n",
    "                    )\n",
    "print(\"Prompt to get solution graph:\\n\")\n",
    "print(solution.direct_request_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcde2502-d8b0-444f-ad5c-076d3fb68843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Response object type: <class 'str'>\n",
      "Debug: Response content: ```python\n",
      "import pandas as pd\n",
      "import geopandas as gpd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy import stats\n",
      "import contextily as ctx\n",
      "\n",
      "def direct_solution():\n",
      "    # 1. Load COVID-19 data\n",
      "    covid_data_url = \"https://github.com/nytimes/covid-19-data/raw/master/us-counties-2020.csv\"\n",
      "    covid_data = pd.read_csv(covid_data_url, dtype={'fips': str})\n",
      "    covid_data = covid_data.dropna(subset=['fips', 'cases', 'deaths'])\n",
      "    last_day_data = covid_data[covid_data['date'] == '2020-12-31'].copy()\n",
      "    last_day_data['death_rate'] = last_day_data['deaths'] / last_day_data['cases']\n",
      "    last_day_data['fips'] = last_day_data['fips'].astype(str).str.zfill(5)\n",
      "\n",
      "\n",
      "    # 2. Load county boundary shapefile\n",
      "    counties_url = \"https://github.com/gladcolor/spatial_data/raw/master/contiguous_counties.zip\"\n",
      "    counties = gpd.read_file(counties_url)\n",
      "    counties = counties.to_crs(epsg=3300) #change to Conus Albers\n",
      "\n",
      "\n",
      "    # 3. Load census data\n",
      "    census_url = \"https://raw.githubusercontent.com/gladcolor/spatial_data/master/Demography/ACS2020_5year_county.csv\"\n",
      "    census_data = pd.read_csv(census_url, dtype={'FIPS': str})\n",
      "    census_data = census_data[['FIPS', 'Total Population', 'Total Population: 65 to 74 Years', \n",
      "                               'Total Population: 75 to 84 Years', 'Total Population: 85 Years and Over']]\n",
      "    census_data = census_data.dropna()\n",
      "    census_data['FIPS'] = census_data['FIPS'].astype(str).str.zfill(5)\n",
      "    census_data['senior_rate'] = (census_data['Total Population: 65 to 74 Years'] + \n",
      "                                 census_data['Total Population: 75 to 84 Years'] + \n",
      "                                 census_data['Total Population: 85 Years and Over']) / census_data['Total Population']\n",
      "\n",
      "\n",
      "    # 4. Join data\n",
      "    merged_data = counties.merge(last_day_data, left_on='GEOID', right_on='fips', how='left')\n",
      "    merged_data = merged_data.merge(census_data, left_on='GEOID', right_on='FIPS', how='left')\n",
      "    merged_data = merged_data.dropna(subset=['death_rate', 'senior_rate'])\n",
      "\n",
      "\n",
      "    # 5. Choropleth map\n",
      "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
      "    merged_data.plot(column='death_rate', cmap='viridis', linewidth=0.8, ax=ax, edgecolor='0.8', scheme='quantiles', legend=True)\n",
      "    ax.set_title('COVID-19 Death Rate (2020-12-31) among Contiguous US Counties', fontsize=16)\n",
      "    ax.set_axis_off()\n",
      "    ctx.add_basemap(ax, crs=merged_data.crs, source=ctx.providers.Esri.WorldGrayCanvas)\n",
      "    plt.savefig(\"covid_choropleth.png\")\n",
      "\n",
      "\n",
      "    # 6. Scatter plot\n",
      "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
      "    sns.regplot(x='senior_rate', y='death_rate', data=merged_data, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n",
      "    slope, intercept, r_value, p_value, std_err = stats.linregress(merged_data['senior_rate'], merged_data['death_rate'])\n",
      "    r_sq = r_value**2\n",
      "    ax.set_title(f'Correlation between Senior Resident Rate and COVID-19 Death Rate (R-squared = {r_sq:.2f}, p-value = {p_value:.3f})', fontsize=16)\n",
      "    ax.set_xlabel('Senior Resident Rate')\n",
      "    ax.set_ylabel('COVID-19 Death Rate')\n",
      "    plt.savefig(\"covid_scatter.png\")\n",
      "\n",
      "```\n",
      "\n",
      "LLM is reviewing the direct request code... \n",
      "\n",
      "Got LLM reply: Several approaches exist to read a CSV file from a URL and store its content in a Python variable.  Here are two common methods, one using the `requests` library and the other using the built-in `urllib.request` library.  Both methods then use the `csv` library to parse the CSV data.\n",
      "\n",
      "**Method 1: Using `requests`**\n",
      "\n",
      "This method is generally preferred for its ease of use and better handling of HTTP requests.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import csv\n",
      "import io\n",
      "\n",
      "url = \"YOUR_CSV_URL_HERE\"  # Replace with the actual URL of your CSV file\n",
      "\n",
      "try:\n",
      "    response = requests.get(url)\n",
      "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
      "\n",
      "    # Read the CSV data into a list of lists (rows)\n",
      "    csv_data = list(csv.reader(io.StringIO(response.text)))\n",
      "\n",
      "    # csv_data now contains the CSV data as a list of lists\n",
      "\n",
      "except requests.exceptions.RequestException as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred during CSV processing: {e}\")\n",
      "\n",
      "\n",
      "# Example of accessing the data:\n",
      "if csv_data:\n",
      "    print(\"First row:\", csv_data[0])\n",
      "    print(\"Number of rows:\", len(csv_data))\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "**Method 2: Using `urllib.request`**\n",
      "\n",
      "This method uses Python's built-in libraries and avoids external dependencies.\n",
      "\n",
      "```python\n",
      "import urllib.request\n",
      "import csv\n",
      "import io\n",
      "\n",
      "url = \"YOUR_CSV_URL_HERE\"  # Replace with the actual URL of your CSV file\n",
      "\n",
      "try:\n",
      "    with urllib.request.urlopen(url) as response:\n",
      "        data = response.read().decode('utf-8') #Decode to handle potential encoding issues\n",
      "        csv_data = list(csv.reader(io.StringIO(data)))\n",
      "\n",
      "    # csv_data now contains the CSV data as a list of lists\n",
      "\n",
      "except urllib.error.URLError as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred during CSV processing: {e}\")\n",
      "\n",
      "# Example of accessing the data:\n",
      "if csv_data:\n",
      "    print(\"First row:\", csv_data[0])\n",
      "    print(\"Number of rows:\", len(csv_data))\n",
      "```\n",
      "\n",
      "**Before running either code snippet:**\n",
      "\n",
      "1. **Replace `\"YOUR_CSV_URL_HERE\"` with the actual URL of your CSV file.**\n",
      "2. **Make sure you have the `requests` library installed if you use Method 1.** You can install it using `pip install requests`.\n",
      "\n",
      "\n",
      "Remember to handle potential errors, such as network issues or invalid CSV formats, as shown in the `try...except` blocks.  The `csv_data` variable will hold your CSV data as a list of lists, where each inner list represents a row in the CSV file.  You can then process this data as needed.\n",
      "\n",
      "Debug: Response object type: <class 'str'>\n",
      "Debug: Response content: Several approaches exist to read a CSV file from a URL and store its content in a Python variable.  Here are two common methods, one using the `requests` library and the other using the built-in `urllib.request` library.  Both methods then use the `csv` library to parse the CSV data.\n",
      "\n",
      "**Method 1: Using `requests`**\n",
      "\n",
      "This method is generally preferred for its ease of use and better handling of HTTP requests.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import csv\n",
      "import io\n",
      "\n",
      "url = \"YOUR_CSV_URL_HERE\"  # Replace with the actual URL of your CSV file\n",
      "\n",
      "try:\n",
      "    response = requests.get(url)\n",
      "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
      "\n",
      "    # Read the CSV data into a list of lists (rows)\n",
      "    csv_data = list(csv.reader(io.StringIO(response.text)))\n",
      "\n",
      "    # csv_data now contains the CSV data as a list of lists\n",
      "\n",
      "except requests.exceptions.RequestException as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred during CSV processing: {e}\")\n",
      "\n",
      "\n",
      "# Example of accessing the data:\n",
      "if csv_data:\n",
      "    print(\"First row:\", csv_data[0])\n",
      "    print(\"Number of rows:\", len(csv_data))\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "**Method 2: Using `urllib.request`**\n",
      "\n",
      "This method uses Python's built-in libraries and avoids external dependencies.\n",
      "\n",
      "```python\n",
      "import urllib.request\n",
      "import csv\n",
      "import io\n",
      "\n",
      "url = \"YOUR_CSV_URL_HERE\"  # Replace with the actual URL of your CSV file\n",
      "\n",
      "try:\n",
      "    with urllib.request.urlopen(url) as response:\n",
      "        data = response.read().decode('utf-8') #Decode to handle potential encoding issues\n",
      "        csv_data = list(csv.reader(io.StringIO(data)))\n",
      "\n",
      "    # csv_data now contains the CSV data as a list of lists\n",
      "\n",
      "except urllib.error.URLError as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred during CSV processing: {e}\")\n",
      "\n",
      "# Example of accessing the data:\n",
      "if csv_data:\n",
      "    print(\"First row:\", csv_data[0])\n",
      "    print(\"Number of rows:\", len(csv_data))\n",
      "```\n",
      "\n",
      "**Before running either code snippet:**\n",
      "\n",
      "1. **Replace `\"YOUR_CSV_URL_HERE\"` with the actual URL of your CSV file.**\n",
      "2. **Make sure you have the `requests` library installed if you use Method 1.** You can install it using `pip install requests`.\n",
      "\n",
      "\n",
      "Remember to handle potential errors, such as network issues or invalid CSV formats, as shown in the `try...except` blocks.  The `csv_data` variable will hold your CSV data as a list of lists, where each inner list represents a row in the CSV file.  You can then process this data as needed.\n",
      "\n",
      "\n",
      "\n",
      "-------------- Running code (trial # 1/10) --------------\n",
      "\n",
      "\n",
      "An error occurred: Invalid URL 'YOUR_CSV_URL_HERE': No scheme supplied. Perhaps you meant https://YOUR_CSV_URL_HERE?\n",
      "Error_info_str: \n",
      "Traceback (most recent call last):\n",
      "  File \"Complete program\", line 23, in <module>\n",
      "    if csv_data:\n",
      "NameError: name 'csv_data' is not defined\n",
      "\n",
      "Sending error information to LLM for debugging...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_LLM_reply() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\Projects\\LLM_Geo_GCP\\LLM_Geo_kernel.py:563\u001b[0m, in \u001b[0;36mSolution.execute_complete_program\u001b[1;34m(self, code, try_cnt)\u001b[0m\n\u001b[0;32m    562\u001b[0m compiled_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcompile\u001b[39m(code, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComplete program\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 563\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiled_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# #pass only globals() not locals()\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m#!!!!    all variables in code will become global variables! May cause huge issues!     !!!!\u001b[39;00m\n",
      "File \u001b[1;32mComplete program:23\u001b[0m\n\u001b[0;32m     17\u001b[0m from vertexai.generative_models import GenerativeModel\n\u001b[0;32m     19\u001b[0m #load config\n\u001b[0;32m     20\u001b[0m # config = configparser.ConfigParser()\n\u001b[0;32m     21\u001b[0m # config.read('config.ini')\n\u001b[0;32m     22\u001b[0m \n\u001b[1;32m---> 23\u001b[0m # use your KEY.\n\u001b[0;32m     24\u001b[0m # OpenAI_key = config.get('API_Key', 'OpenAI_key')\n\u001b[0;32m     25\u001b[0m # client = OpenAI(api_key=OpenAI_key)\n\u001b[0;32m     29\u001b[0m class Solution():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv_data' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m direct_request_LLM_response \u001b[38;5;241m=\u001b[39m solution\u001b[38;5;241m.\u001b[39mget_direct_request_LLM_response(review\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# clear_output(wait=True)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# display(Code(solution.direct_request_code, language='python'))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[43msolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_complete_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirect_request_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_cnt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# solution.direct_request_code = code\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# display(Code(code, language='python'))\u001b[39;00m\n",
      "File \u001b[1;32m~\\Projects\\LLM_Geo_GCP\\LLM_Geo_kernel.py:586\u001b[0m, in \u001b[0;36mSolution.execute_complete_program\u001b[1;34m(self, code, try_cnt)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending error information to LLM for debugging...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;66;03m# print(\"Prompt:\\n\", debug_prompt)\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mhelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_LLM_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msystem_role\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug_role\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerativeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-1.5-flash-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mretry_cnt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m         code \u001b[38;5;241m=\u001b[39m helper\u001b[38;5;241m.\u001b[39mextract_code(response)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "\u001b[1;31mTypeError\u001b[0m: get_LLM_reply() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "direct_request_LLM_response = solution.get_direct_request_LLM_response(review=True)\n",
    "\n",
    "# clear_output(wait=True)\n",
    "# display(Code(solution.direct_request_code, language='python'))\n",
    "\n",
    "code = solution.execute_complete_program(code=solution.direct_request_code, try_cnt=10)\n",
    "# solution.direct_request_code = code\n",
    "# display(Code(code, language='python'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0169fbe-42d9-4f3b-b594-52a57639f71b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b20246-272a-4a80-b875-1c2758f6cdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
